{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy1wDy-iixDB"
   },
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIRlAt1-Bdjo"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wIHrP0kLipan",
    "outputId": "d63fccac-3fb8-4126-efa7-fbeb86abceb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed datasets-4.4.1 pyarrow-22.0.0\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7cb6e394c024581a96e11883b1248773110f093691115f92bef4fc3be31028be\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, evaluate\n",
      "Successfully installed evaluate-0.4.6 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets\n",
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1qCvu06nFWI1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from evaluate import load\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLOhUpdFL0tL",
    "outputId": "37d9ac17-cb03-4d01-9efa-cf288c286553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 13 22:29:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Setting Gpu\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585,
     "referenced_widgets": [
      "1aad61263e5045acbc42483741c1d880",
      "eadfa09e95154280be6ca845fee05976",
      "266945730f2645429ca011ec79a6c081",
      "9117bfae4b754c3395c25ece7be7b5be",
      "699a275a86f9499d996f9c4debee5aee",
      "21e8280a682f498f9d41eaf3ef67963e",
      "6115a4fa4b194e278efd144cb3c45c5e",
      "c335ed222c7c4552be287950198d424b",
      "93701731747f4b18a00897cd9a2a00cc",
      "7ef65b3f7e8d414b947845b8ee3db4d9",
      "0cabae6d52f64f968ce848ebdd8424d3",
      "98d274ab9d434ecf8b842ecb0b2ed62b",
      "98ba401c0435452ba6f17a2deddc8cd3",
      "bb594fbc43b2488b96bc3535c78d80a4",
      "81faa37579d84caba75846d2fd2f9d3e",
      "a2fa033d554f4145a34da6dce2aa192a",
      "ab3f2940f741456fb450626aa7edf7a3",
      "2afa83b059fa49e489c6c570e1952100",
      "a96cec28de1f49f8b5551ca2662668b3",
      "8027be5e0e8d427f99ec8c35f2e54128",
      "2422b9b4595a452490b4b3cf1145a781",
      "77408e0dcee14a06b7e61510863e5964",
      "2a894c0d58fb4a9284604bf636bc1048",
      "5fdc3645712f4946b9147b3d9a5313b4",
      "1fb8196e64df460a8c3b22d65e97df2e",
      "a94fa0f9103b41f6b9c1b571bf64554e",
      "abee7bd63496482e875f34648ddd3a17",
      "3e4f19e83de147d4ada1b8fb628cdb96",
      "593e0cb7960d4dd08bf2d6fb23539d23",
      "d75c260cbf0c407abc6f22ad4d1315cd",
      "a2546736c68a4b9fbcdcafe9c0788bb2",
      "8bd10c4f8bf547b48f34833fe5f2ae80",
      "e017b6832ea1493ca6c3e5db07c00f02",
      "aab29d7ea145400cb2e5c4049b035bea",
      "9fce3b015b0147a99720f0d8b4b6e469",
      "c18f96a339b64afeb01f066b0e766ed2",
      "90ce7dfa72534193a931ea507c38a072",
      "5686c2cc208041919061f4912a3ed7a2",
      "a3b04e40b44247259bab9c597f34b6ee",
      "ef4b88bb13b9430985e793cde1c82783",
      "6cdeb321a0d24219aea238a7075e7beb",
      "71dffceecb5b4c96970ddd1e2a32e6ed",
      "b25cc41a23b84cabad35ea8653750c0b",
      "4038d1e19f4c465d9ee69bcd997b29b9",
      "6ace28b240d54873aee8a575f329377a",
      "e23af583d97c49a7b332856e485d3e57",
      "0936af58b5a5482db149f6e2b969b5ba",
      "f36eafc0b7bb4f0bb730b8e2e87fc1f5",
      "0efef706c8a9442ab1836357c7bf464a",
      "cb35e7a147dc4c1191108fc1e28205fe",
      "5799ce0ee0764639b4550269e36ba950",
      "5a86caedcd754b7c995c1644c3c0b074",
      "6956fa3b94a6429f8c9ca3e3ca9381bd",
      "2e1bc305ddb048658765d7246d1da22a",
      "a292ebd5c0bd4015ab902e4758cf36a7",
      "e5167669ea024c8c99d63d1ce540f268",
      "41fc041c7a9f489198ca8275484756c1",
      "c45955cfb1974eeaadb00040f616f7a0",
      "f53b2f8e108946ba9a0775bb3589903d",
      "ba7bdda8b86c4f12b742fdf9147ec482",
      "55dd7ba4153c438f9c7759c9e1ac64ac",
      "f09fbf9e351242d1a9c230b69abbc053",
      "5bfdd80fb45543a9b5c79af14f723c41",
      "0d25a8afbf94437da95369039f71af50",
      "cdc0ff3ed79648e192a2f2bb455e0536",
      "cedfa0aa957f4293b57df72c14f0f8e2",
      "73784095ed4043779a893def91f60d61",
      "a102a012172a443fbf6ea61a6862b058",
      "f2cc34377370458db154c1b0d7346df0",
      "794cbd1365f7414bbf3fd618a3731dce",
      "170b4bde92614bf397a5e08b9f652f11",
      "0b133f7f635440d092463a90ab547255",
      "61ad96c7351c4d0b9a1697e3aa174d89",
      "e29f58c1d136451ea2b2c76d113736c9",
      "a94c9dbb4e95484f8443443e2581f827",
      "3870a63e8cd843918e135a2e08df713c",
      "06491e6a51e740e9b1fead63a6804eab",
      "7a0a9d69a69f44509bf5d17aa585569d",
      "e4fe6666ffb24a17821569b8336b2d0f",
      "56bf8075f7804ec5957ce101625add0a",
      "f1ae0c40334d4cd98a1ba9f058321f64",
      "aecbbe1aa3f5460e899accfee814ae13",
      "a8aaea454d8d48b48573993ec990be33",
      "dab1f294fcd24066b1300b060cdcfcab",
      "3c7b46f1968b4c118d8cef2742a84b7c",
      "a52820c6c5874c7cbeed4d374ad41103",
      "c5eb35a6cffd47a08cd0ee1e3879d4cf",
      "f1187627da7c4d4e96e5260294285810",
      "b75c7917011744e9b084d63bb7841080",
      "6d52e2bda8c046b399aa4b0012497fd8",
      "275c1f6000b1467b87ecc5ee5d2f706e",
      "ae8232b28c354c4cad906d0dd712a2a2",
      "139c93f6c4b54290a54f11a22d078feb",
      "847c611253b645f3af00854daa68968f",
      "40a2b3938efb44daafa61848f6d45dae",
      "e54056f316d84adc9f532ece76accabd",
      "d8198285e9ae4eb98e40858cf4c38fa4",
      "35f2e48948d74a3e825c6ada9659fffa",
      "63f9df2ca4fc4e5e9276bb4abebcc219",
      "3b7a442ebfbc4f64a60629b2788f3b7f",
      "31f8fe837f0a4a17824f5d45178b0e7f",
      "8495f0afc94b4e50aa397a97e540b34b",
      "af809ca4ae084e488639c8900034fce8",
      "e571d713a6a84c69be0fcc68f3a6b96b",
      "c2bda0ab464e476daa214a0290f63573",
      "c4f42a4889344fa5b877e04c29596db9",
      "d9514802e43d49bf882678b7370d6aeb",
      "f4267f3f97cb4eb9a870ef8a8f8e12b2",
      "6252522a0abe4dbf8bf6c6b06b6fbbbd",
      "33134a27868b4cd49f795267de164919",
      "2af541be20d54bca94d94f961f8eaa64",
      "fee7a7d5a1e64479b98cfd86cd8bc5cf",
      "ad57b6e1656b410697a309d83a4d0d86",
      "49515ea517d2406ebe4b59b577b70c47",
      "7cfef97a46664f5e824ed47f7a470f2c",
      "74e947aa29ad4555908baf4563aa879a",
      "04b51db5c92d4d11978ea3297ea83d97",
      "6c46217fa98b4689a6fad23eb53d8648",
      "dcc5b41a9b9f43e7a7e4af38be8a6865",
      "bf730489bb1b4f70bfbc74dfc6c3c535",
      "005b4e8f298049cbbcf541ac7b1505b9"
     ]
    },
    "id": "yG2aYHym5Vfi",
    "outputId": "5629d0f1-6964-420a-eab8-b76d185ec44a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aad61263e5045acbc42483741c1d880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d274ab9d434ecf8b842ecb0b2ed62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a894c0d58fb4a9284604bf636bc1048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab29d7ea145400cb2e5c4049b035bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ace28b240d54873aee8a575f329377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5167669ea024c8c99d63d1ce540f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73784095ed4043779a893def91f60d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0a9d69a69f44509bf5d17aa585569d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75c7917011744e9b084d63bb7841080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7a442ebfbc4f64a60629b2788f3b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af541be20d54bca94d94f961f8eaa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt before instruction:  LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
      "----------\n",
      "Prompt after instruction:  Provide a short overview of this text.\n",
      "\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n"
     ]
    }
   ],
   "source": [
    "# Load 2% of cnn_dailymail datasets\n",
    "\n",
    "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=[\"train[:2%]\", \"validation[:2%]\"])\n",
    "\n",
    "train, validation = dataset\n",
    "\n",
    "\n",
    "# Add instruction to the prompts\n",
    "prompts = [\n",
    "    \"Summarize this article:\",\n",
    "    \"Write a brief summary of the following news piece:\",\n",
    "    \"Give a concise version of this report:\",\n",
    "    \"In a few sentences, describe the main points of this text:\",\n",
    "    \"What is this story about? Summarize below:\",\n",
    "    \"Condense the following passage:\",\n",
    "    \"Provide a short overview of this text.\"\n",
    "]\n",
    "\n",
    "def add_instruction(example):\n",
    "  prompt = random.choice(prompts)\n",
    "  return {\n",
    "      \"prompt\": f\"{prompt}\\n\\n{example[\"article\"]}\",\n",
    "      \"target\": f\"{example[\"highlights\"]}\"\n",
    "  }\n",
    "\n",
    "train_dataset = train.map(\n",
    "    add_instruction,\n",
    "    remove_columns=train.column_names\n",
    "    )\n",
    "validation_dataset = validation.map(\n",
    "    add_instruction,\n",
    "    remove_columns=validation.column_names\n",
    "    )\n",
    "\n",
    "print(\"Prompt before instruction: \", train[0][\"article\"])\n",
    "print(\"-\"*10)\n",
    "print(\"Prompt after instruction: \", train_dataset[0][\"prompt\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mL88MEoG5gzj"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from evaluate import load\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Seting the finetuning function\n",
    "def fine_tune(batch_size,lr,peft_config,accumulation_steps):\n",
    "\n",
    "  model_name = 't5-base'\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "  model = get_peft_model(model, peft_config)\n",
    "\n",
    "  # Tokenize the datasets\n",
    "  max_input_length = 512\n",
    "  max_target_length = 128\n",
    "\n",
    "  def preprocess_function(examples):\n",
    "      inputs = [ex for ex in examples[\"prompt\"]]\n",
    "      model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "      with tokenizer.as_target_tokenizer():\n",
    "          labels = tokenizer(examples[\"target\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "      model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "      return model_inputs\n",
    "\n",
    "  tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['prompt', 'target'])\n",
    "  tokenized_validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=['prompt', 'target'])\n",
    "\n",
    "  #compute validation ROUGE-1\n",
    "  rouge = load(\"rouge\")\n",
    "\n",
    "  def compute_metrics(eval_pred):\n",
    "      predictions, labels = eval_pred\n",
    "      # Convert -100 to tokenizer.pad_token_id for proper decoding\n",
    "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "      decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "      # Compute ROUGE\n",
    "      result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "      return {\"rouge-1\": result[\"rouge1\"]}\n",
    "\n",
    "  # Defining early_stopping_patience\n",
    "  early_stopping_patience = 2\n",
    "\n",
    "  training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./temp_lr{lr}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=10, #this is the max as I added early stop\n",
    "        gradient_accumulation_steps=accumulation_steps,\n",
    "        weight_decay=0.01,\n",
    "        # save_total_limit=2,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"rouge-1\", # Added metric\n",
    "        greater_is_better=True\n",
    "    )\n",
    "\n",
    "  # Defining data collator\n",
    "  data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "  trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_validation_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "  )\n",
    "  trainer.train()\n",
    "  eval_results = trainer.evaluate()\n",
    "  print(f\"Validation ROUGE-1 for lr={lr}, batch_size={batch_size},accumulation_steps={accumulation_steps},weight_decay={0.01}: \\n {eval_results['eval_rouge-1']}\")\n",
    "\n",
    "\n",
    "\n",
    "#setting LoRA with rate as 8\n",
    "peft_config_eight = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv-HSroVBnyS"
   },
   "source": [
    "# Tune Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U-UcZplCFBF"
   },
   "source": [
    "I’ll keep the following parameters fixed for now:\n",
    "**batch_size = 4, LoRA rank = 8, epochs = 10, and accumulation_steps = 1**, while experimenting with different **learning rates (1e-5, 3e-5, 5e-5)** to find the most effective one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o0g_k2BO0iA"
   },
   "source": [
    "---\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657,
     "referenced_widgets": [
      "7d39989db28d45238a5c18519f85f385",
      "816d27a2b18349dda15d2b7187bcf499",
      "dd5d12fd448a49e787305536d563a905",
      "0c05acdc2b624a51924b1bf1f2dd6a3c",
      "8be3fd55492245719931353017b13004",
      "b698064f0eb047e69147f819c56f0847",
      "8ef0872d196247a8b818ddd60fdd9cc9",
      "97bf0fada7eb4fd196aeca7632895c80",
      "d8a3ba1e6ac54716864966c1162c66ff",
      "d0c963656fcb4184952b1727b3c0cb00",
      "b596f823c82e45db98f9480a8450ba77",
      "cd370aba15fc4a95892d49a2a1749f8a",
      "b1f25ae8231846959b9d3da3347e4d5a",
      "69482d5d23b744439a14bcda46febf30",
      "e71e1f543b2f45eebca5cc357683e036",
      "cffaaf67a3064116ac15b1857bbc0865",
      "95e4dec0238c4315b55c77c80f8d028a",
      "b9e4083f968a43d8be920b17bf98a382",
      "6b4faa7c762841648d45484a12f607a0",
      "f68e8e30b4bb4062a3a9ccca5139b2e4",
      "e01a54450a6046fe9314634684f82058",
      "e6cfa312f64d46d687e30e4fd7f6ca7f",
      "681b67591bd94af3a23ab49dd335c1ff",
      "2297d489cb6043df8e30eabb478a5747",
      "26f541caa2384bb68412b68140aca652",
      "58cf4b81963243998ffdcf36ca3c5520",
      "926e311d6a7f43aa8e298b50f8668361",
      "83806ab32e4f4cf6ba66b430dc01f2f8",
      "c99027c2d6df463f86071280536ff01a",
      "cba4e0cc2fb1427ab1cdb90fd8c4aef1",
      "59742fea709d4cc39ccbce7fe83332fe",
      "120314864db74f62a07ffa05f7da5e5a",
      "52d117981a924595aa7e94ea9184fc0b",
      "160da3c4f9dc47ce97a71e9f59c4019f",
      "ed1cfef840c1416f8c115a4cf8b9e4b9",
      "c72c843222d44105afee18e47bb0c650",
      "dda7127c31b247d2be339b280e1a4c89",
      "b635e60067a54154a725e0c6ac904562",
      "01dce00e483749309a2dd7b0135bf8f2",
      "35bd8627603b4451b32e16225015194c",
      "8c04926a19af470aa43b422f8a27acaf",
      "fce044ece1744a5ea2e6205e47becbeb",
      "95a06daa1e1842b7b603b9b7c91026d2",
      "90edeb6f91a54f7ebfe43dc6848d1f85",
      "b4c1c497a7f94b1387f4c64bee0e2d18",
      "50b347d6b3044b67815083dbc71374a7",
      "bb9d96aa9c954a60af0b95a9fb1641df",
      "87453711db7a4272bf2bcdcebde27473",
      "b0caba089aea4e4e8c00bede53ba5202",
      "1fecda5efada490f97839f0dffea837c",
      "93b4fd256fcd468ca92155af8d4fe50f",
      "72b67249a8674bf997c7f64520daba97",
      "e76c8d33ecd947308e5a4f6a15ed817e",
      "22bc55fa06de482bb8a6edf75a1aedf8",
      "ea4732cbeeb948ffa1319680e2296651",
      "aa166572d2f14829865f9153e24155b0",
      "8e50bb110e5b4883a9f33fc60875aafc",
      "400848a224594af1bc2cbdcfa69bec5b",
      "e84e2f482f0c46b8affac2ebafc1efe0",
      "e45a5894f8bd48e9a1f759d099a03c88",
      "1e46f3a8e60a4853918781f603582386",
      "d8547b1b732a4c9485da26c2118996a3",
      "461cc4ed03c54da39518b3c3049ab5cc",
      "62090fea5f594e53bfaa587a33d269a7",
      "80c5a71968c24d93bd2a53aed1313cb6",
      "b943350d7a8c4f0aaa656892ac02cca8",
      "6b10da2c4319444cb33f1c3fd6851317",
      "6b59e9cbd66047bf8562523823a88e64",
      "82025bdf814a478ea5e676aec1c9d6e8",
      "33f8b7b73eea40389d6fbf1a44999460",
      "e454ae264d324cf9acf5412778ceea14",
      "b40242add08040239e55ba8600c344c6",
      "ee112cc3483a41f588647bc2be1960e0",
      "3f963f980e0f4dad98c5b281025cfda6",
      "575fd532941c40599e33e2e28c6c7cf1",
      "b9a6463116de4851bcfa3a995759c531",
      "5f4b5e1e4c464fda9e5b0da83181e1d4",
      "a3357bb000b342888f8f76e5bea5d55f",
      "43858250a48d48a382ee859bd0b64ae7",
      "df45e5dd37664fac8aa317de6969df98",
      "3ad60f752d374ea7b28b3ea0e1dba3aa",
      "a50a11fee68a418d8b0cb623840b1604",
      "787420e671854928ac2cbb7c0fc15b9f",
      "ab7349c1a9194655af1d439343beeacb",
      "b9ac16b6884c4bd9b6c28810813c6a62",
      "83a6a0b79ce140b79d194ca6351c6ce1",
      "fffc371532d64ede81129d245738575e",
      "58055203f7fe4f069ac6f2b5e91f65b6"
     ]
    },
    "id": "OAR2agKVBzIU",
    "outputId": "a237b435-2f53-4882-b1be-af34f77bb9fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39989db28d45238a5c18519f85f385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd370aba15fc4a95892d49a2a1749f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b67591bd94af3a23ab49dd335c1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160da3c4f9dc47ce97a71e9f59c4019f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1c497a7f94b1387f4c64bee0e2d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa166572d2f14829865f9153e24155b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b10da2c4319444cb33f1c3fd6851317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3357bb000b342888f8f76e5bea5d55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5744' max='14360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5744/14360 28:29 < 42:45, 3.36 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.908900</td>\n",
       "      <td>1.871376</td>\n",
       "      <td>0.263175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.716300</td>\n",
       "      <td>1.830692</td>\n",
       "      <td>0.265220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.682400</td>\n",
       "      <td>1.816001</td>\n",
       "      <td>0.257298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.667100</td>\n",
       "      <td>1.810794</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=4,accumulation_steps=1,weight_decay=0.01: \n",
      " 0.25459951989500085\n"
     ]
    }
   ],
   "source": [
    "fine_tune(4,1e-5,peft_config_eight,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIE2QZYOVSRQ"
   },
   "source": [
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464,
     "referenced_widgets": [
      "c2f29900b9594a33a0a458c9e6e53f61",
      "c9d3b28783814158b422fb5d2dbdeac4",
      "a7955deb51054079a849e2fbf57b8578",
      "79d24b4584224f518aadbcbc806bbec2",
      "4bbdb81651434c9e8b0b22615fe11e7c",
      "5582c822c01048929d658cf040c6069c",
      "b10f3e8a475a4122b73af91219d1199f",
      "1813b717cbd4448c8d6e29f2f73df739",
      "8c0436e6c9bc44a490e81e3611d68c55",
      "c6c0da0d6f6a4ccdad4fb5a39c66e34e",
      "38b448af7b72461aa7ac553e89b4df58"
     ]
    },
    "id": "biViYalkVNp-",
    "outputId": "fb0dcf48-ba65-47db-b25a-fadc78cded74"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f29900b9594a33a0a458c9e6e53f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7180' max='14360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7180/14360 37:34 < 37:34, 3.18 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.769000</td>\n",
       "      <td>1.813428</td>\n",
       "      <td>0.257172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655600</td>\n",
       "      <td>1.801338</td>\n",
       "      <td>0.255606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.640600</td>\n",
       "      <td>1.794509</td>\n",
       "      <td>0.257737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.629900</td>\n",
       "      <td>1.789100</td>\n",
       "      <td>0.257407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.624900</td>\n",
       "      <td>1.775777</td>\n",
       "      <td>0.256216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=3e-05, batch_size=4,accumulation_steps=1,weight_decay=0.01: \n",
      " 0.25621649177643746\n"
     ]
    }
   ],
   "source": [
    "fine_tune(4,3e-5,peft_config_eight,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHEtD_1VVcuq"
   },
   "source": [
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "nimI8NPAVhHF",
    "outputId": "ffaf8070-b9e0-4b0b-9433-60892b4993a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8616' max='14360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8616/14360 44:33 < 29:43, 3.22 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.731100</td>\n",
       "      <td>1.804798</td>\n",
       "      <td>0.254555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.640400</td>\n",
       "      <td>1.791813</td>\n",
       "      <td>0.259105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.624300</td>\n",
       "      <td>1.780619</td>\n",
       "      <td>0.261167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.610100</td>\n",
       "      <td>1.777755</td>\n",
       "      <td>0.262772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.608700</td>\n",
       "      <td>1.771676</td>\n",
       "      <td>0.259212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.612700</td>\n",
       "      <td>1.767221</td>\n",
       "      <td>0.258643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=5e-05, batch_size=4,accumulation_steps=1,weight_decay=0.01: \n",
      " 0.25864261096588614\n"
     ]
    }
   ],
   "source": [
    "fine_tune(4,5e-5,peft_config_eight,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or-u2NgLWDGg"
   },
   "source": [
    "After evaluating multiple learning rate configurations, I select the one that achieves **the highest validation ROUGE-1 score**, indicating better text generation quality.\n",
    "Among all tested values, LR = 1e-5 achieves the best performance with a **ROUGE-1 score of 0.265220**, and therefore chosen as the optimal learning rate for the following experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkbJMx9sXQ0E"
   },
   "source": [
    "# Tune Effective Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EECtLOCcaxdq"
   },
   "source": [
    "I’ll keep the following parameters fixed for now:\n",
    "**learning rate = 1e-5 (previously chosen), LoRA rank = 8, epochs = 10**, while experimenting with different combinations of **batch size** and **accumulation steps** to find the most effective setup.\n",
    "\n",
    "Since effective_batch_size = batch_size × accumulation_steps, I’ll test the following configurations:\n",
    "\n",
    "* **batch_size = 4, accumulation_steps = 2** → effective batch size = **8**\n",
    "\n",
    "* **batch_size = 4, accumulation_steps = 4** → effective batch size = **16**\n",
    "\n",
    "* **batch_size = 8, accumulation_steps = 4** → effective batch size = **32**\n",
    "\n",
    "In the previous experiment, I used **batch_size = 4** and **accumulation_steps = 1** (effective batch size = **4**).\n",
    "I’ll compare its **validation ROUGE-1** score with the three new configurations above to determine which combination achieves the best overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vsm5Acc4eVd6"
   },
   "source": [
    "---\n",
    "batch_size = 4 and accumulation_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "dXsZyVYnekfr",
    "outputId": "d50840bd-7fa9-4379-d5ef-da2a9aa10910"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2872' max='7180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2872/7180 27:32 < 41:21, 1.74 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.991500</td>\n",
       "      <td>1.953368</td>\n",
       "      <td>0.256513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.778300</td>\n",
       "      <td>1.859512</td>\n",
       "      <td>0.266284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.718200</td>\n",
       "      <td>1.835846</td>\n",
       "      <td>0.263855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.692900</td>\n",
       "      <td>1.824021</td>\n",
       "      <td>0.261560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=4,accumulation_steps=2,weight_decay=0.01: \n",
      " 0.26155974915243557\n"
     ]
    }
   ],
   "source": [
    "fine_tune(4,1e-5,peft_config_eight,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp1B7PkUelAd"
   },
   "source": [
    "batch_size = 4 and accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "GCMxJYOGel6j",
    "outputId": "1ea0395f-53a9-43e1-dfc5-692f11fe2fd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1795' max='3590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1795/3590 34:46 < 34:48, 0.86 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.066900</td>\n",
       "      <td>2.083775</td>\n",
       "      <td>0.241863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.890300</td>\n",
       "      <td>1.939577</td>\n",
       "      <td>0.261051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.793000</td>\n",
       "      <td>1.883750</td>\n",
       "      <td>0.262879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.747600</td>\n",
       "      <td>1.859521</td>\n",
       "      <td>0.262715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.726400</td>\n",
       "      <td>1.847185</td>\n",
       "      <td>0.262056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=4,accumulation_steps=4,weight_decay=0.01: \n",
      " 0.2620559091469124\n"
     ]
    }
   ],
   "source": [
    "fine_tune(4,1e-5,peft_config_eight,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCiEEq_DempK"
   },
   "source": [
    "batch_size = 8 and accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "VcRxBIxWenaD",
    "outputId": "02ea5af8-b0e2-453c-f799-c2e2a149f6b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1800 42:31 < 18:15, 0.49 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.125500</td>\n",
       "      <td>2.164186</td>\n",
       "      <td>0.236339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.020100</td>\n",
       "      <td>2.060811</td>\n",
       "      <td>0.241514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.916300</td>\n",
       "      <td>1.976372</td>\n",
       "      <td>0.254107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.859800</td>\n",
       "      <td>1.928532</td>\n",
       "      <td>0.259699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.811800</td>\n",
       "      <td>1.900628</td>\n",
       "      <td>0.262170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.794800</td>\n",
       "      <td>1.883467</td>\n",
       "      <td>0.262151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.771000</td>\n",
       "      <td>1.872873</td>\n",
       "      <td>0.260348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=8,accumulation_steps=4,weight_decay=0.01: \n",
      " 0.2603481027400204\n"
     ]
    }
   ],
   "source": [
    "fine_tune(8,1e-5,peft_config_eight,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RuD-klWeoIC"
   },
   "source": [
    "After evaluating multiple effective batch sizes configurations, I select the one that achieves **the highest validation ROUGE-1 score**.\n",
    "Among all 3 tested values and initial one, Effective batch size = 8 (**batch_size = 4** and **accumulation_steps = 2**) achieves the best performance with a **ROUGE-1 score of 0.266284**, and therefore chosen as the optimal learning rate for the following experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK1JaSaaiuGh"
   },
   "source": [
    "# Tune LoRA Rank (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLoqDdxtjOPK"
   },
   "source": [
    "I’ll keep the following parameters fixed for now:\n",
    "**batch_size = 4 (previously chosen), learning_rate= 1e-5 (already chosen), epochs = 10, and accumulation_steps = 2 (previously chosen)**, while experimenting with different **LoRA ranks (16, 32)** to find the most effective one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JptBW6vCj4CT"
   },
   "source": [
    "---\n",
    "r = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "_S178KDvj4Td",
    "outputId": "e56596f4-faf4-4feb-9094-50bd9c965563"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2872' max='7180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2872/7180 27:34 < 41:23, 1.73 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.991800</td>\n",
       "      <td>1.954624</td>\n",
       "      <td>0.256144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.860013</td>\n",
       "      <td>0.265204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.719000</td>\n",
       "      <td>1.836577</td>\n",
       "      <td>0.264929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.693000</td>\n",
       "      <td>1.824672</td>\n",
       "      <td>0.261892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=4,accumulation_steps=2,weight_decay=0.01: \n",
      " 0.26189172518486653\n"
     ]
    }
   ],
   "source": [
    "#setting LoRA with rate as 16\n",
    "peft_config_sixteen = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "fine_tune(4,1e-5,peft_config_sixteen,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I8HUa4Bj4ot"
   },
   "source": [
    "r = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640,
     "referenced_widgets": [
      "6b4009f2a6494c998785551f348e702e",
      "54a42f81e9494d7ba03e964cfb9000ef",
      "818a8cb232364d55b1bda15fcaa36bb6",
      "d79dec86037b4ab0a7714950a93db4f5",
      "6d3dc7ea8d2149ec8764e7bb16e64f89",
      "c4a6c6c035204b3cb59f8564f5b39d90",
      "28a805aacc2040058e06d74ce7073d14",
      "939b4f810f1247aea19a8c10be172078",
      "4245e3c390ba4a6b8dff161ee48cb4a1",
      "ff983c16c9ee4f6f9817ef4fc4682b83",
      "cb7dbd5c64b14665b8903640336ac20c",
      "4dfb1b5bba1540fcb53dfb372563bea6",
      "5e685ecf67de44ea934c169f9975e54f",
      "ff58e2af4ba8476c820b4c0842ca8b2c",
      "cb567a16f33440d390eca5c5b9d0dfcb",
      "714cc3ca69d44f7daeaa1aceef52c377",
      "69dd4cbd87d241a5a951388ffc81a566",
      "91a9966b8d1440958b1fdcba635916c9",
      "933cd8965f1c4502a1fa05c7e3deab82",
      "cce207cfdf134d399cf022d421042b3f",
      "7cf709bf195b4125b0703d65ad96ccd8",
      "60ab1fcf3f7747d09c899ecfeefc383b",
      "2dadd7ac4c1b4d74a19386322887212e",
      "b54ba367c4714abc8013f0a5def66627",
      "01f780a073ab4ae29af5bcdb42d56b2c",
      "80aaebbf7e3440f2858d736055a077fe",
      "02b2240395ca41f0bd7a278b193ff584",
      "fcbc6cff2a3f428ea9d9dee34de0fae5",
      "ba8002b4847c4332939f0d02858b9103",
      "e64474228c2648f8bf7f4154b52fc488",
      "ef7dd4e4d6d14e77929168fb15f7a8a9",
      "1ce28e3672614f8f9ef51a5945cdadb5",
      "0f06bef36ef14424b682e2472a12e671",
      "7ac9a34a409a4ba597f06c90567ed4d2",
      "f899ed450c5b4ed2b00c1992a3e4e8de",
      "b3f3e44cbd69415d8a621194af6549ab",
      "00636093a7154eceb2b18ee107adc198",
      "e5d39d553d5241d0bc2530a51b3d492c",
      "89cd48622d004acf87fd70d740fb55dd",
      "19e26745fe344baa9793bc352aa634cd",
      "d85c2d5a194d428cb89d1ddba49c920f",
      "ee28486fb0af468886a7b116dd5746ad",
      "2012c51892d446f9b1fadc1b5b4bffc0",
      "fe59e56114c44aa2969df0d8aeb2359d",
      "948a96e6c5c646a8a35ce644e410fdf7",
      "55aead472155428883e5bace651dff25",
      "caa78eabd55b42d4a0ad53c4e2b233eb",
      "172b6d46e47742c1850ef6886ad99821",
      "746c5b2894cc4dcd8a6ce39d1d3582e9",
      "ec63493125444996bccd7e368ade556b",
      "b242780d3fe44597ba6422e4a5bc8bbc",
      "b03fd36d87bb4f558adde2fffbea84f7",
      "e58a53ffb4dd4d90b15ce1bde8d3f0c8",
      "4afa0454e8cf44feb26477f4b8919890",
      "887cd8a2440c4263a8f57720aa5edebc",
      "8b245c253ff04e5c8f9987f0923aabe8",
      "18c19415777f40a2ae03247e4b8ef5f9",
      "fe7d3f10c9ab47469f14f71f01840e3e",
      "f70bb6c6a4f848a1a300a683e07a90bc",
      "28390ba1c2474547a5a683c7ed6b60ea",
      "36bc9359480b417d87896a11768eeb91",
      "443634b8796249d19902f0eb8adcf3a6",
      "5f0487b528d3483dac531462e9b46dab",
      "ccae0129c6014935a8a434a22d4968a4",
      "62cea757c2284a24abf4d7eecfaaea96",
      "e65140e623a24ff196fb4a482e057bd1",
      "970d2567be394744be733bdca213bdcb",
      "62b750ff0e124c778885afe6aa37684f",
      "4438788e18d54b5193389c8a06aab733",
      "6c0595e12cc8434ba89bf3ece4be7c5b",
      "ab1d1b2f136649d8bf2cf125b766de9d",
      "d34a3d3b015f4cbca3f4a5f83e8fc780",
      "de0620622e974dd482c75394c6ffcb0e",
      "d3da69611a48492ab9195c0a16e1651b",
      "44e282b494534c6f89debf705cd52e3e",
      "eb329e4589d04212a3097ba3c02443b6",
      "e9498be3d6e74e9bb38b6fad580291b1",
      "eba75acec8de448b9e16b4d45cb3835f",
      "f2bb2be472ca459ca5dab4971ffe0466",
      "c8c8893584184ccbb0f58e5830640424",
      "4fa525f2bedb491dadad806ffb265c5c",
      "7ec69b67b5c94031804b577ec3952f5e",
      "2df771ff36b348e385175187c4734124",
      "5edaf13a3c6e4bd2ae8047527a049772",
      "f1c0f6f85ffe4a859ef47f36d664b5ca",
      "434d1b4f06e74d49a0f4c3ede82e4b9a",
      "2ca08fca7560427fb73b8d749cafd61b",
      "af4aae5ba3ed4f66ba4f2695371085c9"
     ]
    },
    "id": "do0e6oTAj47X",
    "outputId": "d2481451-2606-40d4-b1b0-cd4b8a75376f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4009f2a6494c998785551f348e702e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfb1b5bba1540fcb53dfb372563bea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dadd7ac4c1b4d74a19386322887212e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac9a34a409a4ba597f06c90567ed4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948a96e6c5c646a8a35ce644e410fdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b245c253ff04e5c8f9987f0923aabe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d2567be394744be733bdca213bdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba75acec8de448b9e16b4d45cb3835f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-1418846444.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2872' max='7180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2872/7180 27:58 < 41:59, 1.71 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.987000</td>\n",
       "      <td>1.956399</td>\n",
       "      <td>0.250877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.777700</td>\n",
       "      <td>1.861669</td>\n",
       "      <td>0.265536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.718100</td>\n",
       "      <td>1.838071</td>\n",
       "      <td>0.264944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.693900</td>\n",
       "      <td>1.826390</td>\n",
       "      <td>0.261876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE-1 for lr=1e-05, batch_size=4,accumulation_steps=2,weight_decay=0.01: \n",
      " 0.2618761559566565\n"
     ]
    }
   ],
   "source": [
    "#setting LoRA with rate as 32\n",
    "peft_config_thirtyTwo = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "fine_tune(4,1e-5,peft_config_thirtyTwo,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT9BmXW8j5Of"
   },
   "source": [
    "After evaluating multiple LoRA ranks configurations (initial r=8, then 16 and 32), I select the one that achieves **the highest validation ROUGE-1 score**.\n",
    "\n",
    "Among all tested values, r = 8 (initial one) achieves the best performance with a **ROUGE-1 score of 0.266284**, and therefore chosen as the optimal LoRA rank."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "czBzGKhjgbiR"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "> # Final Conclusion\n",
    "\n",
    "\n",
    "\n",
    "I used **2% of the CNN/DailyMail dataset** to tune the hyperparameters for LoRA fine-tuning on T5-Base.\n",
    "Across all experiments, I selected the configuration that achieved the **highest and most stable validation ROUGE-1 score**, while maintaining stable training dynamics and no overfitting.\n",
    "\n",
    "**Best hyperparameter combination found:**\n",
    "\n",
    "* **Batch Size:** 4\n",
    "\n",
    "* **Accumulation Steps:** 2 (effective batch size = 8)\n",
    "\n",
    "* **Learning Rate:** 1e-5\n",
    "\n",
    "* **LoRA Rank (r):** 8\n",
    "\n",
    "* **Max Epochs:** 10 (with early stopping)\n",
    "\n",
    "This setup provided the **best trade-off between generalization, and compute efficiency.**\n",
    "\n",
    "I will use this hyperparameter combination when fine-tuning the T5 base model with **instruction augmentation.**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0 (main, Nov  2 2023, 21:43:31) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
