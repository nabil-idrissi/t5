**Disclaimer:**
This notebook finetunes T5 on **2% of CNN/DailyMail** due to compute limits.
**The full training code** is identical for the entire dataset; only hyperparameters need to be rescaled.
Although this small subset is not sufficient for a high-quality summarization model, it allowed me to explore **the real finetuning workflow**, LR behavior, hyperparameter ranges, and loss dynamics.
